{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MI_Assignment.pynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7zJvXyCcFBr9qDDZmG3bK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedicarao/ANN-for-LBW-cases-/blob/main/MI_Assignment_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41OzX31VIM3b"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJT95tisIOcb"
      },
      "source": [
        "df = pd.read_csv(\"LBW_Dataset_clean.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_340ms9YIOgk"
      },
      "source": [
        "df = df.drop(['Education'], axis=1)\n",
        "#df = df.drop(['Residence'], axis=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnlrWrK_OWkE"
      },
      "source": [
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YErIHFpJOWpy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsIItkwGOWiU"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra-SkhN22Kq9",
        "outputId": "b3120243-2f26-477a-ece4-e99c9c616d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLvsjJa9IlRy"
      },
      "source": [
        "# train=df.sample(frac=0.8,random_state=50) #random state is a seed value\n",
        "# test=df.drop(train.index)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsKmxrw0IlQJ"
      },
      "source": [
        "# x_train = train.iloc[:, :-1].values\n",
        "# y_train = train.iloc[:, -1].values\n",
        "# x_test = test.iloc[:, :-1].values\n",
        "# y_test = test.iloc[:, -1].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ZEaIVlKlQp"
      },
      "source": [
        "x_train_reshaped=X_train.reshape((8,76))\n",
        "y_train_reshaped=y_train.reshape((1,76))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MWw7pGXIObh"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# define activation functions g(Z), and their first derivative, using numpy\n",
        "# hold all activation function pairs in a global dict\n",
        "logistic = lambda Z: 1/(1+np.exp(-Z.clip(-708, 709)))\n",
        "def logisticprime(Z):\n",
        "    s = logistic(Z)\n",
        "    return 2 * s * (1-s)\n",
        "\n",
        "relu = lambda Z: np.maximum(0, Z)\n",
        "reluprime = lambda Z: (Z > 0).astype(float)\n",
        "\n",
        "tanh = lambda Z: np.tanh(Z)\n",
        "tanhprime = lambda Z: logisticprime(2*Z) * 2\n",
        "\n",
        "elu = lambda Z: np.where(Z>0, Z, np.exp(Z)-1)\n",
        "eluprime = lambda Z: np.where(Z>0, 1, np.exp(Z))\n",
        "\n",
        "ACTFUNC = { #    function  differentiation\n",
        "    'logistic': (logistic, logisticprime),\n",
        "    'relu':     (relu,     reluprime),\n",
        "    'tanh':     (tanh,     tanhprime),\n",
        "    'elu':      (elu,      eluprime),\n",
        "}\n",
        "\n",
        "# Loss functions L(Y, Y_hat) and their partial derivative\n",
        "def xentropy(Y, Y_hat):\n",
        "    \"\"\"Binary cross entropy function\n",
        "        L = - Y log Y_hat - (1-Y) log (1-Y_hat)\n",
        "    Args:\n",
        "        Y, Y_hat (np.array): nxm matrices which n are the number of data instances and m the number\n",
        "                of output perceptons\n",
        "    \"\"\"\n",
        "    eps = np.finfo(float).eps\n",
        "    return -(np.dot(Y, np.log(Y_hat.clip(eps)).T) + np.dot(1-Y, np.log((1-Y_hat).clip(eps)).T)) / Y.shape[1]\n",
        "def xentropyprime(Y, Y_hat):\n",
        "    \"\"\" dL/dY_hat \"\"\"\n",
        "    eps = np.finfo(float).eps\n",
        "    return - np.divide(Y, Y_hat.clip(eps)) + np.divide(1-Y, (1-Y_hat).clip(eps))\n",
        "\n",
        "LOSSFUNC = {\n",
        "        'xentropy': (xentropy, xentropyprime)\n",
        "}\n",
        "\n",
        "class pyann:\n",
        "    '''Artificial Neural Network using numpy\n",
        "    '''\n",
        "    def __init__(self, layersizes, activations, lossfunc='xentropy'):\n",
        "        \"\"\"remember config, then initialize array to hold NN parameters without init\"\"\"\n",
        "        # hold NN config\n",
        "        self.layersizes = tuple(layersizes)\n",
        "        self.activations = tuple(activations)\n",
        "        self.lossfunc = lossfunc\n",
        "        assert len(self.layersizes)-1 == len(self.activations), \\\n",
        "            \"NN number of layers and the activation function spec does not match\"\n",
        "        assert all(f in ACTFUNC for f in activations), \"Unrecognized activation function used\"\n",
        "        assert all(isinstance(n, int) and n >= 1 for n in layersizes), \\\n",
        "            \"Only positive integral number of perceptons is allowed in each layer\"\n",
        "        assert lossfunc in LOSSFUNC, \\\n",
        "            \"Unrecognized loss function used\"\n",
        "        # parameters, each is a 2D numpy array\n",
        "        L = len(self.layersizes)\n",
        "        self.Z = [None] * L\n",
        "        self.W = [None] * L\n",
        "        self.b = [None] * L\n",
        "        self.A = [None] * L\n",
        "        self.dZ = [None] * L\n",
        "        self.dW = [None] * L\n",
        "        self.db = [None] * L\n",
        "        self.dA = [None] * L\n",
        "\n",
        "    def init_nn(self, seed=35):\n",
        "        \"\"\"init the value of weight matrices and bias vectors with small random numbers. We do not\n",
        "        use real truncated normal but a plain normal with clip at 6 sigmas. We assume we use\n",
        "        activation functions that will have large derivatives around 0 so init value concentrated\n",
        "        around 0 will speed up learning\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        sigma = 0.1\n",
        "        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n",
        "            self.W[l] = np.random.randn(outsize, insize).clip(-6, 6) * sigma\n",
        "            self.b[l] = np.random.randn(outsize, 1).clip(-6, 6) * sigma\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Feed forward the NN using existing W and b, and overwrite the result variables A and Z\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data to feed forward\n",
        "        \"\"\"\n",
        "        self.A[0] = X\n",
        "        for l, funcname in enumerate(self.activations, 1):\n",
        "            # Z = W A + b, with A as output from previous layer\n",
        "            # W is of size rxs and A the size sxn with n the number of data instances, Z the size rxn\n",
        "            # b is rx1 and broadcast to each column of Z\n",
        "            g = ACTFUNC[funcname][0]\n",
        "            self.Z[l] = np.dot(self.W[l], self.A[l-1]) + self.b[l]\n",
        "            # A = g(Z), with A as output of this layer, of size rxn\n",
        "            self.A[l] = g(self.Z[l])\n",
        "        return self.A[-1]\n",
        "\n",
        "    def backward(self, Y, Y_hat):\n",
        "        \"\"\"back propagation using NN output Y_hat and the reference output Y, generates dW, dZ, db,\n",
        "        dA\n",
        "        \"\"\"\n",
        "        assert Y.shape[0] == self.layersizes[-1], \"Output size mismatch NN\"\n",
        "        assert Y.shape == Y_hat.shape, \"Output size mismatch reference\"\n",
        "        # first dA, at the output\n",
        "        self.dA[-1] = LOSSFUNC[self.lossfunc][1](Y, Y_hat)\n",
        "        for l, funcname in reversed(list(enumerate(self.activations, 1))):\n",
        "            m = self.layersizes[l]\n",
        "            g_prime = ACTFUNC[funcname][1]\n",
        "            # compute the differentials at this layer\n",
        "            self.dZ[l] = self.dA[l] * g_prime(self.Z[l])\n",
        "            self.dW[l] = np.dot(self.dZ[l], self.A[l-1].T) / m\n",
        "            self.db[l] = np.sum(self.dZ[l], axis=1, keepdims=True) / m\n",
        "            self.dA[l-1] = np.dot(self.W[l].T, self.dZ[l])\n",
        "\n",
        "    def update(self, alpha):\n",
        "        \"\"\"Updates W and b\n",
        "        Args:\n",
        "            alpha (float): Learning rate\n",
        "        \"\"\"\n",
        "        for l in range(1, len(self.W)):\n",
        "            self.W[l] -= alpha * self.dW[l]\n",
        "            self.b[l] -= alpha * self.db[l]\n",
        "\n",
        "        # for t in range(1, len(self.W)):\n",
        "        #     g = compute_gradient(x, y)\n",
        "        #     m = beta_1 * m + (1 - beta_1) * g\n",
        "        #     v = beta_2 * v + (1 - beta_2) * np.power(g, 2)\n",
        "        #     m_hat = m / (1 - np.power(beta_1, t)) + (1 - beta_1) * g / (1 - np.power(beta_1, t))\n",
        "        #     v_hat = v / (1 - np.power(beta_2, t))\n",
        "        #     self.W = W - step_size * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "                     s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "    def initialize_adam(self):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    \n",
        "    return v, s\n",
        "    \n",
        "    def convert_prob_into_class(self, probs):\n",
        "        probs_ = np.copy(probs)\n",
        "        probs_[probs_ > 0.5] = 1\n",
        "        probs_[probs_ <= 0.5] = 0\n",
        "        return probs_\n",
        "\n",
        "    def get_accuracy_value(self, Y_hat, Y):\n",
        "        Y_hat_ = self.convert_prob_into_class(Y_hat)\n",
        "        return (Y_hat_ == Y).all(axis=0).mean()\n",
        "\n",
        "    def fit(self, X, Y, epochs, alpha, printfreq=1):\n",
        "        \"\"\"Train a NN\n",
        "        Args:\n",
        "            X: input data, of size mxn which n is the number of data instances and m the number of\n",
        "                features\n",
        "            Y: reference output, of size nxm which n is the number of data instances and m the size\n",
        "                of each output\n",
        "            alpha: the learning rate\n",
        "            epochs:\n",
        "        \"\"\"\n",
        "        self.init_nn()\n",
        "        lossfunc = LOSSFUNC[self.lossfunc][0]\n",
        "        # train for each epoch\n",
        "        for j in range(epochs):\n",
        "            self.forward(X)\n",
        "            Y_hat = self.A[-1]\n",
        "            self.backward(Y, Y_hat)\n",
        "            self.update(alpha)\n",
        "            if printfreq and j % printfreq == 0:\n",
        "                loss = float(lossfunc(Y, Y_hat))\n",
        "                accuracy = self.get_accuracy_value(Y_hat, Y)\n",
        "                print(\"Iteration {} - loss value {} - acc value {}\".format(j, loss, accuracy))\n",
        "        # report loss value\n",
        "        accuracy = self.get_accuracy_value(Y_hat, Y)\n",
        "        return lossfunc(Y, Y_hat), accuracy\n",
        "\n",
        "      def predict\n",
        "\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROZLb0RLI1Nx"
      },
      "source": [
        "ann=pyann([8,4,6,6,6,4,1],['relu', 'relu','relu','relu','relu','logistic'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F92stsGI1M4",
        "outputId": "d02f4755-deee-4b00-b996-9a818a3f5403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ann.fit(x_train_reshaped,y_train_reshaped,20,0.01)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 - loss value 0.6804881934931959 - acc value 0.7631578947368421\n",
            "Iteration 1 - loss value 0.6028405490018381 - acc value 0.7631578947368421\n",
            "Iteration 2 - loss value 0.5719839494383603 - acc value 0.7631578947368421\n",
            "Iteration 3 - loss value 0.5588704378781477 - acc value 0.7631578947368421\n",
            "Iteration 4 - loss value 0.5529506433082215 - acc value 0.7631578947368421\n",
            "Iteration 5 - loss value 0.5501567503354889 - acc value 0.7631578947368421\n",
            "Iteration 6 - loss value 0.5487960449772237 - acc value 0.7631578947368421\n",
            "Iteration 7 - loss value 0.5481185332160381 - acc value 0.7631578947368421\n",
            "Iteration 8 - loss value 0.5477758877552282 - acc value 0.7631578947368421\n",
            "Iteration 9 - loss value 0.547600668248748 - acc value 0.7631578947368421\n",
            "Iteration 10 - loss value 0.5475103550754995 - acc value 0.7631578947368421\n",
            "Iteration 11 - loss value 0.5474635408495184 - acc value 0.7631578947368421\n",
            "Iteration 12 - loss value 0.5474391755803993 - acc value 0.7631578947368421\n",
            "Iteration 13 - loss value 0.5474264570391293 - acc value 0.7631578947368421\n",
            "Iteration 14 - loss value 0.5474198039713268 - acc value 0.7631578947368421\n",
            "Iteration 15 - loss value 0.5474163184285719 - acc value 0.7631578947368421\n",
            "Iteration 16 - loss value 0.5474144903322622 - acc value 0.7631578947368421\n",
            "Iteration 17 - loss value 0.5474135307651893 - acc value 0.7631578947368421\n",
            "Iteration 18 - loss value 0.547413026796724 - acc value 0.7631578947368421\n",
            "Iteration 19 - loss value 0.5474127619992742 - acc value 0.7631578947368421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.54741276]]), 0.7631578947368421)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg7pKmAf1sPx",
        "outputId": "bf1b2dcd-52fa-44bf-d682-1902e2329dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "print(ann.fit(x_train_reshaped,y_train_reshaped,200,0.001))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 - loss value 0.6804881934931959 - acc value 0.7631578947368421\n",
            "Iteration 1 - loss value 0.6711053146647066 - acc value 0.7631578947368421\n",
            "Iteration 2 - loss value 0.6624209102441657 - acc value 0.7631578947368421\n",
            "Iteration 3 - loss value 0.654381861469977 - acc value 0.7631578947368421\n",
            "Iteration 4 - loss value 0.6469388341347722 - acc value 0.7631578947368421\n",
            "Iteration 5 - loss value 0.6400460930849733 - acc value 0.7631578947368421\n",
            "Iteration 6 - loss value 0.6336613057319544 - acc value 0.7631578947368421\n",
            "Iteration 7 - loss value 0.627745340271515 - acc value 0.7631578947368421\n",
            "Iteration 8 - loss value 0.6222620631177208 - acc value 0.7631578947368421\n",
            "Iteration 9 - loss value 0.6171781390339193 - acc value 0.7631578947368421\n",
            "Iteration 10 - loss value 0.6124628365800653 - acc value 0.7631578947368421\n",
            "Iteration 11 - loss value 0.6080878407784361 - acc value 0.7631578947368421\n",
            "Iteration 12 - loss value 0.6040270743137806 - acc value 0.7631578947368421\n",
            "Iteration 13 - loss value 0.6002565281122975 - acc value 0.7631578947368421\n",
            "Iteration 14 - loss value 0.5967541017703125 - acc value 0.7631578947368421\n",
            "Iteration 15 - loss value 0.5934994540127485 - acc value 0.7631578947368421\n",
            "Iteration 16 - loss value 0.5904738631395323 - acc value 0.7631578947368421\n",
            "Iteration 17 - loss value 0.5876600972526385 - acc value 0.7631578947368421\n",
            "Iteration 18 - loss value 0.5850422939368124 - acc value 0.7631578947368421\n",
            "Iteration 19 - loss value 0.5826058489840499 - acc value 0.7631578947368421\n",
            "Iteration 20 - loss value 0.5803373136979189 - acc value 0.7631578947368421\n",
            "Iteration 21 - loss value 0.5782243002824561 - acc value 0.7631578947368421\n",
            "Iteration 22 - loss value 0.576255394806406 - acc value 0.7631578947368421\n",
            "Iteration 23 - loss value 0.5744200772327033 - acc value 0.7631578947368421\n",
            "Iteration 24 - loss value 0.5727086480119792 - acc value 0.7631578947368421\n",
            "Iteration 25 - loss value 0.571112160754668 - acc value 0.7631578947368421\n",
            "Iteration 26 - loss value 0.5696223605169412 - acc value 0.7631578947368421\n",
            "Iteration 27 - loss value 0.5682316272594513 - acc value 0.7631578947368421\n",
            "Iteration 28 - loss value 0.5669329240634746 - acc value 0.7631578947368421\n",
            "Iteration 29 - loss value 0.5657197497154725 - acc value 0.7631578947368421\n",
            "Iteration 30 - loss value 0.5645860952976598 - acc value 0.7631578947368421\n",
            "Iteration 31 - loss value 0.5635264044482711 - acc value 0.7631578947368421\n",
            "Iteration 32 - loss value 0.5625355369805195 - acc value 0.7631578947368421\n",
            "Iteration 33 - loss value 0.5616087355734679 - acc value 0.7631578947368421\n",
            "Iteration 34 - loss value 0.560741595270963 - acc value 0.7631578947368421\n",
            "Iteration 35 - loss value 0.5599300355464303 - acc value 0.7631578947368421\n",
            "Iteration 36 - loss value 0.5591702747114812 - acc value 0.7631578947368421\n",
            "Iteration 37 - loss value 0.558458806465135 - acc value 0.7631578947368421\n",
            "Iteration 38 - loss value 0.5577923783978448 - acc value 0.7631578947368421\n",
            "Iteration 39 - loss value 0.5571679722806153 - acc value 0.7631578947368421\n",
            "Iteration 40 - loss value 0.5565827859843142 - acc value 0.7631578947368421\n",
            "Iteration 41 - loss value 0.5560342168878742 - acc value 0.7631578947368421\n",
            "Iteration 42 - loss value 0.5555198466465505 - acc value 0.7631578947368421\n",
            "Iteration 43 - loss value 0.5550374272028091 - acc value 0.7631578947368421\n",
            "Iteration 44 - loss value 0.554584867932844 - acc value 0.7631578947368421\n",
            "Iteration 45 - loss value 0.5541602238312238 - acc value 0.7631578947368421\n",
            "Iteration 46 - loss value 0.5537616846448551 - acc value 0.7631578947368421\n",
            "Iteration 47 - loss value 0.5533875648753328 - acc value 0.7631578947368421\n",
            "Iteration 48 - loss value 0.5530362945759538 - acc value 0.7631578947368421\n",
            "Iteration 49 - loss value 0.5527064108762072 - acc value 0.7631578947368421\n",
            "Iteration 50 - loss value 0.5523965501725041 - acc value 0.7631578947368421\n",
            "Iteration 51 - loss value 0.5521054409293343 - acc value 0.7631578947368421\n",
            "Iteration 52 - loss value 0.5518318970399463 - acc value 0.7631578947368421\n",
            "Iteration 53 - loss value 0.5515748117001303 - acc value 0.7631578947368421\n",
            "Iteration 54 - loss value 0.551333151752743 - acc value 0.7631578947368421\n",
            "Iteration 55 - loss value 0.551105952464322 - acc value 0.7631578947368421\n",
            "Iteration 56 - loss value 0.5508923126984866 - acc value 0.7631578947368421\n",
            "Iteration 57 - loss value 0.5506913904538919 - acc value 0.7631578947368421\n",
            "Iteration 58 - loss value 0.5505023987372666 - acc value 0.7631578947368421\n",
            "Iteration 59 - loss value 0.5503246017446151 - acc value 0.7631578947368421\n",
            "Iteration 60 - loss value 0.5501573113259507 - acc value 0.7631578947368421\n",
            "Iteration 61 - loss value 0.5499998837110232 - acc value 0.7631578947368421\n",
            "Iteration 62 - loss value 0.5498517164754171 - acc value 0.7631578947368421\n",
            "Iteration 63 - loss value 0.5497122457281343 - acc value 0.7631578947368421\n",
            "Iteration 64 - loss value 0.5495809435033469 - acc value 0.7631578947368421\n",
            "Iteration 65 - loss value 0.5494573153404613 - acc value 0.7631578947368421\n",
            "Iteration 66 - loss value 0.549340898037945 - acc value 0.7631578947368421\n",
            "Iteration 67 - loss value 0.5492312575675654 - acc value 0.7631578947368421\n",
            "Iteration 68 - loss value 0.5491279871367869 - acc value 0.7631578947368421\n",
            "Iteration 69 - loss value 0.5490307053880753 - acc value 0.7631578947368421\n",
            "Iteration 70 - loss value 0.5489390547247658 - acc value 0.7631578947368421\n",
            "Iteration 71 - loss value 0.548852699753991 - acc value 0.7631578947368421\n",
            "Iteration 72 - loss value 0.5487713258379248 - acc value 0.7631578947368421\n",
            "Iteration 73 - loss value 0.5486946377452951 - acc value 0.7631578947368421\n",
            "Iteration 74 - loss value 0.5486223583957667 - acc value 0.7631578947368421\n",
            "Iteration 75 - loss value 0.5485542276903571 - acc value 0.7631578947368421\n",
            "Iteration 76 - loss value 0.5484900014216185 - acc value 0.7631578947368421\n",
            "Iteration 77 - loss value 0.5484294502577717 - acc value 0.7631578947368421\n",
            "Iteration 78 - loss value 0.54837235879546 - acc value 0.7631578947368421\n",
            "Iteration 79 - loss value 0.5483185246761807 - acc value 0.7631578947368421\n",
            "Iteration 80 - loss value 0.5482677577618462 - acc value 0.7631578947368421\n",
            "Iteration 81 - loss value 0.5482198793652636 - acc value 0.7631578947368421\n",
            "Iteration 82 - loss value 0.5481747215316489 - acc value 0.7631578947368421\n",
            "Iteration 83 - loss value 0.5481321263675794 - acc value 0.7631578947368421\n",
            "Iteration 84 - loss value 0.548091945414062 - acc value 0.7631578947368421\n",
            "Iteration 85 - loss value 0.5480540390606441 - acc value 0.7631578947368421\n",
            "Iteration 86 - loss value 0.5480182759977131 - acc value 0.7631578947368421\n",
            "Iteration 87 - loss value 0.5479845327043584 - acc value 0.7631578947368421\n",
            "Iteration 88 - loss value 0.5479526929693432 - acc value 0.7631578947368421\n",
            "Iteration 89 - loss value 0.5479226474429296 - acc value 0.7631578947368421\n",
            "Iteration 90 - loss value 0.5478942932174503 - acc value 0.7631578947368421\n",
            "Iteration 91 - loss value 0.5478675334346889 - acc value 0.7631578947368421\n",
            "Iteration 92 - loss value 0.5478422769182518 - acc value 0.7631578947368421\n",
            "Iteration 93 - loss value 0.5478184378292605 - acc value 0.7631578947368421\n",
            "Iteration 94 - loss value 0.5477959353438037 - acc value 0.7631578947368421\n",
            "Iteration 95 - loss value 0.5477746933506994 - acc value 0.7631578947368421\n",
            "Iteration 96 - loss value 0.5477546401682291 - acc value 0.7631578947368421\n",
            "Iteration 97 - loss value 0.5477357082785809 - acc value 0.7631578947368421\n",
            "Iteration 98 - loss value 0.5477178340788496 - acc value 0.7631578947368421\n",
            "Iteration 99 - loss value 0.5477009576475033 - acc value 0.7631578947368421\n",
            "Iteration 100 - loss value 0.5476850225253124 - acc value 0.7631578947368421\n",
            "Iteration 101 - loss value 0.5476699755098015 - acc value 0.7631578947368421\n",
            "Iteration 102 - loss value 0.5476557664623497 - acc value 0.7631578947368421\n",
            "Iteration 103 - loss value 0.5476423481271252 - acc value 0.7631578947368421\n",
            "Iteration 104 - loss value 0.547629675961096 - acc value 0.7631578947368421\n",
            "Iteration 105 - loss value 0.5476177079744077 - acc value 0.7631578947368421\n",
            "Iteration 106 - loss value 0.547606404580469 - acc value 0.7631578947368421\n",
            "Iteration 107 - loss value 0.5475957284551289 - acc value 0.7631578947368421\n",
            "Iteration 108 - loss value 0.5475856444043709 - acc value 0.7631578947368421\n",
            "Iteration 109 - loss value 0.5475761192399865 - acc value 0.7631578947368421\n",
            "Iteration 110 - loss value 0.5475671216627314 - acc value 0.7631578947368421\n",
            "Iteration 111 - loss value 0.5475586221524886 - acc value 0.7631578947368421\n",
            "Iteration 112 - loss value 0.5475505928650102 - acc value 0.7631578947368421\n",
            "Iteration 113 - loss value 0.5475430075348211 - acc value 0.7631578947368421\n",
            "Iteration 114 - loss value 0.5475358413839093 - acc value 0.7631578947368421\n",
            "Iteration 115 - loss value 0.5475290710358397 - acc value 0.7631578947368421\n",
            "Iteration 116 - loss value 0.5475226744349633 - acc value 0.7631578947368421\n",
            "Iteration 117 - loss value 0.5475166307704007 - acc value 0.7631578947368421\n",
            "Iteration 118 - loss value 0.5475109204045194 - acc value 0.7631578947368421\n",
            "Iteration 119 - loss value 0.5475055248056179 - acc value 0.7631578947368421\n",
            "Iteration 120 - loss value 0.5475004264845701 - acc value 0.7631578947368421\n",
            "Iteration 121 - loss value 0.5474956089351836 - acc value 0.7631578947368421\n",
            "Iteration 122 - loss value 0.5474910565780524 - acc value 0.7631578947368421\n",
            "Iteration 123 - loss value 0.5474867547076867 - acc value 0.7631578947368421\n",
            "Iteration 124 - loss value 0.5474826894427282 - acc value 0.7631578947368421\n",
            "Iteration 125 - loss value 0.5474788476790625 - acc value 0.7631578947368421\n",
            "Iteration 126 - loss value 0.5474752170456552 - acc value 0.7631578947368421\n",
            "Iteration 127 - loss value 0.547471785862951 - acc value 0.7631578947368421\n",
            "Iteration 128 - loss value 0.5474685431036778 - acc value 0.7631578947368421\n",
            "Iteration 129 - loss value 0.5474654783559219 - acc value 0.7631578947368421\n",
            "Iteration 130 - loss value 0.547462581788328 - acc value 0.7631578947368421\n",
            "Iteration 131 - loss value 0.5474598441173076 - acc value 0.7631578947368421\n",
            "Iteration 132 - loss value 0.5474572565761332 - acc value 0.7631578947368421\n",
            "Iteration 133 - loss value 0.5474548108858069 - acc value 0.7631578947368421\n",
            "Iteration 134 - loss value 0.5474524992275979 - acc value 0.7631578947368421\n",
            "Iteration 135 - loss value 0.5474503142171565 - acc value 0.7631578947368421\n",
            "Iteration 136 - loss value 0.5474482488801047 - acc value 0.7631578947368421\n",
            "Iteration 137 - loss value 0.5474462966290219 - acc value 0.7631578947368421\n",
            "Iteration 138 - loss value 0.5474444512417432 - acc value 0.7631578947368421\n",
            "Iteration 139 - loss value 0.547442706840893 - acc value 0.7631578947368421\n",
            "Iteration 140 - loss value 0.5474410578745834 - acc value 0.7631578947368421\n",
            "Iteration 141 - loss value 0.5474394990982069 - acc value 0.7631578947368421\n",
            "Iteration 142 - loss value 0.5474380255572656 - acc value 0.7631578947368421\n",
            "Iteration 143 - loss value 0.5474366325711694 - acc value 0.7631578947368421\n",
            "Iteration 144 - loss value 0.547435315717954 - acc value 0.7631578947368421\n",
            "Iteration 145 - loss value 0.5474340708198621 - acc value 0.7631578947368421\n",
            "Iteration 146 - loss value 0.5474328939297379 - acc value 0.7631578947368421\n",
            "Iteration 147 - loss value 0.5474317813181905 - acc value 0.7631578947368421\n",
            "Iteration 148 - loss value 0.5474307294614793 - acc value 0.7631578947368421\n",
            "Iteration 149 - loss value 0.5474297350300827 - acc value 0.7631578947368421\n",
            "Iteration 150 - loss value 0.5474287948779085 - acc value 0.7631578947368421\n",
            "Iteration 151 - loss value 0.5474279060321111 - acc value 0.7631578947368421\n",
            "Iteration 152 - loss value 0.54742706568348 - acc value 0.7631578947368421\n",
            "Iteration 153 - loss value 0.5474262711773672 - acc value 0.7631578947368421\n",
            "Iteration 154 - loss value 0.5474255200051229 - acc value 0.7631578947368421\n",
            "Iteration 155 - loss value 0.5474248097960112 - acc value 0.7631578947368421\n",
            "Iteration 156 - loss value 0.5474241383095768 - acc value 0.7631578947368421\n",
            "Iteration 157 - loss value 0.5474235034284394 - acc value 0.7631578947368421\n",
            "Iteration 158 - loss value 0.5474229031514893 - acc value 0.7631578947368421\n",
            "Iteration 159 - loss value 0.5474223355874631 - acc value 0.7631578947368421\n",
            "Iteration 160 - loss value 0.5474217989488789 - acc value 0.7631578947368421\n",
            "Iteration 161 - loss value 0.5474212915463058 - acc value 0.7631578947368421\n",
            "Iteration 162 - loss value 0.5474208117829572 - acc value 0.7631578947368421\n",
            "Iteration 163 - loss value 0.5474203581495801 - acc value 0.7631578947368421\n",
            "Iteration 164 - loss value 0.5474199292196318 - acc value 0.7631578947368421\n",
            "Iteration 165 - loss value 0.5474195236447233 - acc value 0.7631578947368421\n",
            "Iteration 166 - loss value 0.5474191401503153 - acc value 0.7631578947368421\n",
            "Iteration 167 - loss value 0.547418777531654 - acc value 0.7631578947368421\n",
            "Iteration 168 - loss value 0.5474184346499301 - acc value 0.7631578947368421\n",
            "Iteration 169 - loss value 0.5474181104286548 - acc value 0.7631578947368421\n",
            "Iteration 170 - loss value 0.547417803850231 - acc value 0.7631578947368421\n",
            "Iteration 171 - loss value 0.5474175139527184 - acc value 0.7631578947368421\n",
            "Iteration 172 - loss value 0.5474172398267767 - acc value 0.7631578947368421\n",
            "Iteration 173 - loss value 0.5474169806127754 - acc value 0.7631578947368421\n",
            "Iteration 174 - loss value 0.5474167354980671 - acc value 0.7631578947368421\n",
            "Iteration 175 - loss value 0.5474165037144084 - acc value 0.7631578947368421\n",
            "Iteration 176 - loss value 0.547416284535524 - acc value 0.7631578947368421\n",
            "Iteration 177 - loss value 0.5474160772748059 - acc value 0.7631578947368421\n",
            "Iteration 178 - loss value 0.5474158812831386 - acc value 0.7631578947368421\n",
            "Iteration 179 - loss value 0.5474156959468442 - acc value 0.7631578947368421\n",
            "Iteration 180 - loss value 0.5474155206857414 - acc value 0.7631578947368421\n",
            "Iteration 181 - loss value 0.547415354951311 - acc value 0.7631578947368421\n",
            "Iteration 182 - loss value 0.5474151982249615 - acc value 0.7631578947368421\n",
            "Iteration 183 - loss value 0.547415050016392 - acc value 0.7631578947368421\n",
            "Iteration 184 - loss value 0.5474149098620436 - acc value 0.7631578947368421\n",
            "Iteration 185 - loss value 0.5474147773236355 - acc value 0.7631578947368421\n",
            "Iteration 186 - loss value 0.5474146519867843 - acc value 0.7631578947368421\n",
            "Iteration 187 - loss value 0.5474145334596957 - acc value 0.7631578947368421\n",
            "Iteration 188 - loss value 0.5474144213719306 - acc value 0.7631578947368421\n",
            "Iteration 189 - loss value 0.547414315373237 - acc value 0.7631578947368421\n",
            "Iteration 190 - loss value 0.5474142151324484 - acc value 0.7631578947368421\n",
            "Iteration 191 - loss value 0.5474141203364391 - acc value 0.7631578947368421\n",
            "Iteration 192 - loss value 0.5474140306891409 - acc value 0.7631578947368421\n",
            "Iteration 193 - loss value 0.5474139459106105 - acc value 0.7631578947368421\n",
            "Iteration 194 - loss value 0.5474138657361495 - acc value 0.7631578947368421\n",
            "Iteration 195 - loss value 0.5474137899154723 - acc value 0.7631578947368421\n",
            "Iteration 196 - loss value 0.5474137182119198 - acc value 0.7631578947368421\n",
            "Iteration 197 - loss value 0.5474136504017151 - acc value 0.7631578947368421\n",
            "Iteration 198 - loss value 0.5474135862732616 - acc value 0.7631578947368421\n",
            "Iteration 199 - loss value 0.5474135256264786 - acc value 0.7631578947368421\n",
            "(array([[0.54741353]]), 0.7631578947368421)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWTp0IVQ3Hom"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}